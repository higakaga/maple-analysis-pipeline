version: "3.8"

services:
  airflow-init:
    build: .
    entrypoint: ""
    command: bash -lc "
      airflow db migrate && \
      airflow users create \
        --username admin \
        --password admin \
        --firstname basic \
        --lastname Basic \
        --role Admin \
        --email admin@example.com || true && \

      airflow connections add 'maple_postgres' \
        --conn-type 'postgres' \
        --conn-host ${RDS_HOST} \
        --conn-login ${RDS_USER} \
        --conn-password ${RDS_PASSWORD} \
        --conn-schema ${RDS_DB} \
        --conn-port 5432 || true && \

      airflow variables set MAPLE_S3_BUCKET ${MAPLE_S3_BUCKET} && \
      airflow variables set MAPLE_RDS_CONN_ID maple_postgres && \
      airflow variables set MAPLE_MAX_QPS 400 && \
      airflow variables set MAPLE_NUM_WORKERS 10
      "
    restart: no
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
  airflow-scheduler:
    build: .
    command: scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW__CORE__PARALLELISM=2
      - AIRFLOW__CORE__DAG_CONCURRENCY=1
      - AIRFLOW__SCHEDULER__PARSING_PROCESSES=1
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
